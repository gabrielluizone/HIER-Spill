{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Documents\\server-defell\\GroundingDINO\n",
      "c:\\Users\\Usuario\\Documents\\server-defell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas padrão\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import base64\n",
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# Bibliotecas de terceiros\n",
    "import numpy as np\n",
    "import torch\n",
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "\n",
    "# Imports para modelos de ML\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import supervision as sv\n",
    "from datumaro import Bbox, Dataset, DatasetItem\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('oil_detection_server.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuração de caminhos para GroundingDINO\n",
    "HOME = os.getcwd()\n",
    "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO\", \"groundingdino\", \"config\", \"GroundingDINO_SwinT_OGC.py\")\n",
    "%cd {HOME}/GroundingDINO\n",
    "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
    "WEIGHTS_PATH = os.path.join(HOME, \"GroundingDINO\\\\weights\", WEIGHTS_NAME)\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "%cd {HOME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"Gerenciador do banco de dados SQLite com estrutura normalizada por empresa\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"Overseas\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        self._databases = {}  # Cache de conexões por empresa\n",
    "    \n",
    "    def get_db_path(self, empresa_id: str) -> Path:\n",
    "        \"\"\"Retorna o caminho do banco de dados específico da empresa\"\"\"\n",
    "        empresa_dir = self.base_path / empresa_id\n",
    "        empresa_dir.mkdir(exist_ok=True)\n",
    "        return empresa_dir / f\"{empresa_id}_detection.db\"\n",
    "    \n",
    "    def init_database(self, empresa_id: str):\n",
    "        \"\"\"Inicializa o banco de dados específico da empresa com as tabelas normalizadas\"\"\"\n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Tabela de empresa (uma única empresa por banco)\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS empresa (\n",
    "                empresa_id TEXT PRIMARY KEY,\n",
    "                empresa_nome TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de coletores da empresa\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS coletores (\n",
    "                coletor_id TEXT PRIMARY KEY,\n",
    "                coletor_descricao TEXT,\n",
    "                coletor_localizacao TEXT,\n",
    "                status TEXT DEFAULT 'ativo',\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela principal de detecções\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS deteccoes (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                coletor_id TEXT NOT NULL,\n",
    "                timestamp_coleta TIMESTAMP NOT NULL,\n",
    "                timestamp_processamento TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                modelo_usado TEXT NOT NULL,\n",
    "                imagem_original_path TEXT NOT NULL,\n",
    "                imagem_processada_path TEXT,\n",
    "                confidence_yolo REAL,\n",
    "                objects_detected_yolo INTEGER DEFAULT 0,\n",
    "                FOREIGN KEY (coletor_id) REFERENCES coletores (coletor_id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela para captions do Florence-2 (normalizada)\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS florence_captions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                deteccao_id INTEGER NOT NULL,\n",
    "                caption_type TEXT NOT NULL,\n",
    "                caption_text TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (deteccao_id) REFERENCES deteccoes (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela para regiões densas do Florence-2\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS florence_dense_regions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                deteccao_id INTEGER NOT NULL,\n",
    "                region_text TEXT,\n",
    "                bbox_x1 REAL,\n",
    "                bbox_y1 REAL,\n",
    "                bbox_x2 REAL,\n",
    "                bbox_y2 REAL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (deteccao_id) REFERENCES deteccoes (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela para objetos detectados pelo Florence-2\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS florence_objects (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                deteccao_id INTEGER NOT NULL,\n",
    "                object_label TEXT,\n",
    "                bbox_x1 REAL,\n",
    "                bbox_y1 REAL,\n",
    "                bbox_x2 REAL,\n",
    "                bbox_y2 REAL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (deteccao_id) REFERENCES deteccoes (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela principal para resultados do GroundingDINO\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS grounding_dino_results (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                deteccao_id INTEGER NOT NULL,\n",
    "                oil_spill_detected BOOLEAN NOT NULL,\n",
    "                max_confidence REAL,\n",
    "                total_detections INTEGER DEFAULT 0,\n",
    "                image_width INTEGER,\n",
    "                image_height INTEGER,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (deteccao_id) REFERENCES deteccoes (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela para detecções individuais do GroundingDINO\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS grounding_detections (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                grounding_result_id INTEGER NOT NULL,\n",
    "                phrase TEXT,\n",
    "                confidence REAL,\n",
    "                bbox_x1 REAL,\n",
    "                bbox_y1 REAL,\n",
    "                bbox_x2 REAL,\n",
    "                bbox_y2 REAL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (grounding_result_id) REFERENCES grounding_dino_results (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Criar índices para melhor performance\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_deteccoes_coletor ON deteccoes(coletor_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_deteccoes_timestamp ON deteccoes(timestamp_coleta)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_deteccoes_modelo ON deteccoes(modelo_usado)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_florence_captions_deteccao ON florence_captions(deteccao_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_florence_captions_type ON florence_captions(caption_type)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_florence_regions_deteccao ON florence_dense_regions(deteccao_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_florence_objects_deteccao ON florence_objects(deteccao_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_grounding_results_deteccao ON grounding_dino_results(deteccao_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_grounding_detections_result ON grounding_detections(grounding_result_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_grounding_detections_confidence ON grounding_detections(confidence)')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        logger.info(f\"Normalized database initialized successfully for empresa: {empresa_id}\")\n",
    "    \n",
    "    def insert_empresa(self, empresa_id: str, empresa_nome: str):\n",
    "        \"\"\"Insere ou atualiza dados da empresa em seu próprio banco\"\"\"\n",
    "        self.init_database(empresa_id)  # Garante que o banco existe\n",
    "        \n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO empresa (empresa_id, empresa_nome, updated_at)\n",
    "            VALUES (?, ?, CURRENT_TIMESTAMP)\n",
    "        ''', (empresa_id, empresa_nome))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def insert_coletor(self, empresa_id: str, coletor_id: str, descricao: str, localizacao: str):\n",
    "        \"\"\"Insere ou atualiza coletor no banco da empresa\"\"\"\n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO coletores \n",
    "            (coletor_id, coletor_descricao, coletor_localizacao, updated_at)\n",
    "            VALUES (?, ?, ?, CURRENT_TIMESTAMP)\n",
    "        ''', (coletor_id, descricao, localizacao))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def insert_deteccao(self, empresa_id: str, dados_deteccao: Dict) -> int:\n",
    "        \"\"\"Insere uma nova detecção no banco da empresa e retorna o ID\"\"\"\n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT INTO deteccoes \n",
    "            (coletor_id, timestamp_coleta, modelo_usado, \n",
    "             imagem_original_path, imagem_processada_path, confidence_yolo, objects_detected_yolo)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            dados_deteccao['coletor_id'],\n",
    "            dados_deteccao['timestamp_coleta'],\n",
    "            dados_deteccao['modelo_usado'],\n",
    "            dados_deteccao['imagem_original_path'],\n",
    "            dados_deteccao.get('imagem_processada_path'),\n",
    "            dados_deteccao.get('confidence_yolo'),\n",
    "            dados_deteccao.get('objects_detected_yolo', 0)\n",
    "        ))\n",
    "        \n",
    "        deteccao_id = cursor.lastrowid\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return deteccao_id\n",
    "    \n",
    "    def insert_florence_result(self, empresa_id: str, deteccao_id: int, florence_data: Dict):\n",
    "        \"\"\"Insere resultado do Florence-2 de forma normalizada no banco da empresa\"\"\"\n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Inserir captions simples\n",
    "        caption_types = {\n",
    "            '<CAPTION>': 'caption',\n",
    "            '<DETAILED_CAPTION>': 'detailed_caption',\n",
    "            '<MORE_DETAILED_CAPTION>': 'more_detailed_caption'\n",
    "        }\n",
    "        \n",
    "        for key, caption_type in caption_types.items():\n",
    "            caption_text = florence_data.get(key, '')\n",
    "            if caption_text:\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO florence_captions (deteccao_id, caption_type, caption_text)\n",
    "                    VALUES (?, ?, ?)\n",
    "                ''', (deteccao_id, caption_type, caption_text))\n",
    "        \n",
    "        # Inserir regiões densas\n",
    "        dense_regions = florence_data.get('<DENSE_REGION_CAPTION>', {})\n",
    "        if isinstance(dense_regions, dict):\n",
    "            labels = dense_regions.get('labels', [])\n",
    "            bboxes = dense_regions.get('bboxes', [])\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                bbox = bboxes[i] if i < len(bboxes) else [0, 0, 0, 0]\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO florence_dense_regions \n",
    "                    (deteccao_id, region_text, bbox_x1, bbox_y1, bbox_x2, bbox_y2)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                ''', (deteccao_id, label, bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "        \n",
    "        # Inserir objetos detectados\n",
    "        objects_data = florence_data.get('<OD>', {})\n",
    "        if isinstance(objects_data, dict):\n",
    "            labels = objects_data.get('labels', [])\n",
    "            bboxes = objects_data.get('bboxes', [])\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                bbox = bboxes[i] if i < len(bboxes) else [0, 0, 0, 0]\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO florence_objects \n",
    "                    (deteccao_id, object_label, bbox_x1, bbox_y1, bbox_x2, bbox_y2)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                ''', (deteccao_id, label, bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def insert_grounding_result(self, empresa_id: str, deteccao_id: int, grounding_data: Dict):\n",
    "        \"\"\"Insere resultado do GroundingDINO de forma normalizada no banco da empresa\"\"\"\n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        detection_data = grounding_data.get('detection_data', {})\n",
    "        logits = detection_data.get('logits', [])\n",
    "        boxes = detection_data.get('boxes', [])\n",
    "        phrases = detection_data.get('phrases', [])\n",
    "        image_dims = detection_data.get('image_dimensions', (0, 0))\n",
    "        \n",
    "        # Converter numpy arrays para listas Python se necessário\n",
    "        if isinstance(logits, np.ndarray):\n",
    "            logits_list = logits.tolist()\n",
    "            has_detections = logits.size > 0\n",
    "            max_confidence = float(np.max(logits)) if logits.size > 0 else 0.0\n",
    "            total_detections = int(logits.size)\n",
    "        else:\n",
    "            logits_list = list(logits) if logits else []\n",
    "            has_detections = len(logits_list) > 0\n",
    "            max_confidence = float(max(logits_list)) if logits_list else 0.0\n",
    "            total_detections = len(logits_list)\n",
    "        \n",
    "        if isinstance(boxes, np.ndarray):\n",
    "            boxes_list = boxes.tolist()\n",
    "        else:\n",
    "            boxes_list = list(boxes) if boxes else []\n",
    "        \n",
    "        if isinstance(phrases, np.ndarray):\n",
    "            phrases_list = phrases.tolist()\n",
    "        else:\n",
    "            phrases_list = list(phrases) if phrases else []\n",
    "        \n",
    "        # Inserir resultado principal do GroundingDINO\n",
    "        cursor.execute('''\n",
    "            INSERT INTO grounding_dino_results \n",
    "            (deteccao_id, oil_spill_detected, max_confidence, total_detections, image_width, image_height)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            deteccao_id,\n",
    "            has_detections,\n",
    "            max_confidence,\n",
    "            total_detections,\n",
    "            image_dims[1],  # width\n",
    "            image_dims[0]   # height\n",
    "        ))\n",
    "        \n",
    "        grounding_result_id = cursor.lastrowid\n",
    "        \n",
    "        # Inserir detecções individuais\n",
    "        for i, confidence in enumerate(logits_list):\n",
    "            phrase = phrases_list[i] if i < len(phrases_list) else ''\n",
    "            bbox = boxes_list[i] if i < len(boxes_list) else [0, 0, 0, 0]\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT INTO grounding_detections \n",
    "                (grounding_result_id, phrase, confidence, bbox_x1, bbox_y1, bbox_x2, bbox_y2)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                grounding_result_id,\n",
    "                phrase,\n",
    "                float(confidence),\n",
    "                float(bbox[0]),\n",
    "                float(bbox[1]),\n",
    "                float(bbox[2]),\n",
    "                float(bbox[3])\n",
    "            ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def get_detection_with_details(self, empresa_id: str, deteccao_id: int) -> Dict:\n",
    "        \"\"\"Recupera uma detecção completa com todos os detalhes normalizados do banco da empresa\"\"\"\n",
    "        db_path = self.get_db_path(empresa_id)\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Buscar detecção principal com dados da empresa e coletor\n",
    "        cursor.execute('''\n",
    "            SELECT d.*, e.empresa_nome, c.coletor_descricao, c.coletor_localizacao\n",
    "            FROM deteccoes d\n",
    "            JOIN empresa e ON e.empresa_id = ?\n",
    "            JOIN coletores c ON d.coletor_id = c.coletor_id\n",
    "            WHERE d.id = ?\n",
    "        ''', (empresa_id, deteccao_id))\n",
    "        \n",
    "        deteccao = cursor.fetchone()\n",
    "        if not deteccao:\n",
    "            conn.close()\n",
    "            return {}\n",
    "        \n",
    "        # Buscar captions do Florence\n",
    "        cursor.execute('''\n",
    "            SELECT caption_type, caption_text\n",
    "            FROM florence_captions\n",
    "            WHERE deteccao_id = ?\n",
    "        ''', (deteccao_id,))\n",
    "        captions = {row[0]: row[1] for row in cursor.fetchall()}\n",
    "        \n",
    "        # Buscar regiões densas\n",
    "        cursor.execute('''\n",
    "            SELECT region_text, bbox_x1, bbox_y1, bbox_x2, bbox_y2\n",
    "            FROM florence_dense_regions\n",
    "            WHERE deteccao_id = ?\n",
    "        ''', (deteccao_id,))\n",
    "        dense_regions = [\n",
    "            {\n",
    "                'text': row[0],\n",
    "                'bbox': [row[1], row[2], row[3], row[4]]\n",
    "            }\n",
    "            for row in cursor.fetchall()\n",
    "        ]\n",
    "        \n",
    "        # Buscar objetos do Florence\n",
    "        cursor.execute('''\n",
    "            SELECT object_label, bbox_x1, bbox_y1, bbox_x2, bbox_y2\n",
    "            FROM florence_objects\n",
    "            WHERE deteccao_id = ?\n",
    "        ''', (deteccao_id,))\n",
    "        florence_objects = [\n",
    "            {\n",
    "                'label': row[0],\n",
    "                'bbox': [row[1], row[2], row[3], row[4]]\n",
    "            }\n",
    "            for row in cursor.fetchall()\n",
    "        ]\n",
    "        \n",
    "        # Buscar resultados do GroundingDINO\n",
    "        cursor.execute('''\n",
    "            SELECT oil_spill_detected, max_confidence, total_detections, image_width, image_height\n",
    "            FROM grounding_dino_results\n",
    "            WHERE deteccao_id = ?\n",
    "        ''', (deteccao_id,))\n",
    "        grounding_main = cursor.fetchone()\n",
    "        \n",
    "        # Buscar detecções individuais do GroundingDINO\n",
    "        cursor.execute('''\n",
    "            SELECT gd.phrase, gd.confidence, gd.bbox_x1, gd.bbox_y1, gd.bbox_x2, gd.bbox_y2\n",
    "            FROM grounding_detections gd\n",
    "            JOIN grounding_dino_results gdr ON gd.grounding_result_id = gdr.id\n",
    "            WHERE gdr.deteccao_id = ?\n",
    "            ORDER BY gd.confidence DESC\n",
    "        ''', (deteccao_id,))\n",
    "        grounding_detections = [\n",
    "            {\n",
    "                'phrase': row[0],\n",
    "                'confidence': row[1],\n",
    "                'bbox': [row[2], row[3], row[4], row[5]]\n",
    "            }\n",
    "            for row in cursor.fetchall()\n",
    "        ]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Montar resultado completo\n",
    "        return {\n",
    "            'deteccao': {\n",
    "                'id': deteccao[0],\n",
    "                'empresa_id': empresa_id,\n",
    "                'empresa_nome': deteccao[9],\n",
    "                'coletor_id': deteccao[1],\n",
    "                'coletor_descricao': deteccao[10],\n",
    "                'coletor_localizacao': deteccao[11],\n",
    "                'timestamp_coleta': deteccao[2],\n",
    "                'timestamp_processamento': deteccao[3],\n",
    "                'modelo_usado': deteccao[4],\n",
    "                'imagem_original_path': deteccao[5],\n",
    "                'imagem_processada_path': deteccao[6],\n",
    "                'confidence_yolo': deteccao[7],\n",
    "                'objects_detected_yolo': deteccao[8]\n",
    "            },\n",
    "            'florence': {\n",
    "                'captions': captions,\n",
    "                'dense_regions': dense_regions,\n",
    "                'objects': florence_objects\n",
    "            },\n",
    "            'grounding_dino': {\n",
    "                'oil_spill_detected': grounding_main[0] if grounding_main else False,\n",
    "                'max_confidence': grounding_main[1] if grounding_main else 0.0,\n",
    "                'total_detections': grounding_main[2] if grounding_main else 0,\n",
    "                'image_dimensions': [grounding_main[4], grounding_main[3]] if grounding_main else [0, 0],\n",
    "                'detections': grounding_detections\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_empresa_stats(self, empresa_id: str) -> Dict:\n",
    "        \"\"\"Retorna estatísticas específicas da empresa\"\"\"\n",
    "        try:\n",
    "            db_path = self.get_db_path(empresa_id)\n",
    "            if not db_path.exists():\n",
    "                return {\"error\": \"Database not found for empresa\"}\n",
    "                \n",
    "            conn = sqlite3.connect(str(db_path))\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Estatísticas gerais\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM deteccoes\")\n",
    "            total_deteccoes = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.execute(\"SELECT COUNT(*) FROM grounding_dino_results WHERE oil_spill_detected = 1\")\n",
    "            deteccoes_positivas = cursor.fetchone()[0]\n",
    "            \n",
    "            cursor.execute(\"SELECT COUNT(DISTINCT coletor_id) FROM deteccoes\")\n",
    "            total_coletores = cursor.fetchone()[0]\n",
    "            \n",
    "            # Top coletores com mais detecções\n",
    "            cursor.execute('''\n",
    "                SELECT c.coletor_id, c.coletor_descricao, COUNT(d.id) as total_deteccoes\n",
    "                FROM coletores c\n",
    "                LEFT JOIN deteccoes d ON c.coletor_id = d.coletor_id\n",
    "                GROUP BY c.coletor_id\n",
    "                ORDER BY total_deteccoes DESC\n",
    "                LIMIT 5\n",
    "            ''')\n",
    "            top_coletores = cursor.fetchall()\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                \"empresa_id\": empresa_id,\n",
    "                \"total_deteccoes\": total_deteccoes,\n",
    "                \"deteccoes_positivas\": deteccoes_positivas,\n",
    "                \"taxa_deteccao\": (deteccoes_positivas / total_deteccoes * 100) if total_deteccoes > 0 else 0,\n",
    "                \"total_coletores\": total_coletores,\n",
    "                \"top_coletores\": [\n",
    "                    {\n",
    "                        \"coletor_id\": row[0],\n",
    "                        \"descricao\": row[1],\n",
    "                        \"total_deteccoes\": row[2]\n",
    "                    }\n",
    "                    for row in top_coletores\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao obter estatísticas da empresa {empresa_id}: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "class FileManager:\n",
    "    \"\"\"Gerenciador de arquivos e diretórios por empresa\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"Overseas\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def create_directory_structure(self, empresa_id: str, coletor_id: str, modelo: str) -> Tuple[Path, Path]:\n",
    "        \"\"\"Cria estrutura de diretórios específica da empresa e retorna paths para raw e processed\"\"\"\n",
    "        base_dir = self.base_path / empresa_id / coletor_id / modelo\n",
    "        raw_dir = base_dir / \"raw_image\"\n",
    "        processed_dir = base_dir / \"processed\"\n",
    "        \n",
    "        raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "        processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        return raw_dir, processed_dir\n",
    "    \n",
    "    def generate_filename(self, empresa_id: str, coletor_id: str, timestamp: str, \n",
    "                         modelo: str, objects_count: int, prefix: str = \"RAW\") -> str:\n",
    "        \"\"\"Gera nome do arquivo seguindo o padrão especificado\"\"\"\n",
    "        return f\"{prefix}-{empresa_id}-{coletor_id}-{timestamp}-{modelo}-{objects_count}.jpg\"\n",
    "    \n",
    "    def save_image(self, image: Image.Image, filepath: Path) -> bool:\n",
    "        \"\"\"Salva imagem no caminho especificado\"\"\"\n",
    "        try:\n",
    "            image.save(filepath, \"JPEG\", quality=95)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao salvar imagem {filepath}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_empresa_directory_info(self, empresa_id: str) -> Dict:\n",
    "        \"\"\"Retorna informações sobre os diretórios da empresa\"\"\"\n",
    "        empresa_dir = self.base_path / empresa_id\n",
    "        if not empresa_dir.exists():\n",
    "            return {\"error\": \"Empresa directory not found\"}\n",
    "        \n",
    "        info = {\n",
    "            \"empresa_id\": empresa_id,\n",
    "            \"base_path\": str(empresa_dir),\n",
    "            \"coletores\": []\n",
    "        }\n",
    "        \n",
    "        for coletor_dir in empresa_dir.iterdir():\n",
    "            if coletor_dir.is_dir():\n",
    "                coletor_info = {\n",
    "                    \"coletor_id\": coletor_dir.name,\n",
    "                    \"path\": str(coletor_dir),\n",
    "                    \"modelos\": []\n",
    "                }\n",
    "                \n",
    "                for modelo_dir in coletor_dir.iterdir():\n",
    "                    if modelo_dir.is_dir():\n",
    "                        raw_count = len(list((modelo_dir / \"raw_image\").glob(\"*.jpg\"))) if (modelo_dir / \"raw_image\").exists() else 0\n",
    "                        processed_count = len(list((modelo_dir / \"processed\").glob(\"*.jpg\"))) if (modelo_dir / \"processed\").exists() else 0\n",
    "                        \n",
    "                        coletor_info[\"modelos\"].append({\n",
    "                            \"modelo\": modelo_dir.name,\n",
    "                            \"raw_images\": raw_count,\n",
    "                            \"processed_images\": processed_count\n",
    "                        })\n",
    "                \n",
    "                info[\"coletores\"].append(coletor_info)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "class ModelManager:\n",
    "    \"\"\"Gerenciador dos modelos Florence-2 e GroundingDINO\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.florence_processor = None\n",
    "        self.florence_model = None\n",
    "        self.grounding_model = None\n",
    "        self._load_models()\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \"\"\"Carrega os modelos na inicialização\"\"\"\n",
    "        try:\n",
    "            # Carregar Florence-2\n",
    "            logger.info(\"Loading Florence-2 model...\")\n",
    "            from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "            \n",
    "            self.florence_processor = AutoProcessor.from_pretrained(\n",
    "                \"microsoft/florence-2-base-ft\", trust_remote_code=True\n",
    "            )\n",
    "            self.florence_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"microsoft/florence-2-base-ft\", trust_remote_code=True\n",
    "            )\n",
    "            logger.info(\"Florence-2 model loaded successfully\")\n",
    "            \n",
    "            # Carregar GroundingDINO (comentado por enquanto - você precisa ajustar os imports)\n",
    "            logger.info(\"Loading GroundingDINO model...\")\n",
    "            self.grounding_model = load_model(CONFIG_PATH, WEIGHTS_PATH, device='cpu')\n",
    "            logger.info(\"GroundingDINO model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao carregar modelos: {e}\")\n",
    "    \n",
    "    def generate_florence_captions(self, image: Image.Image, max_new_tokens: int = 1024) -> Dict:\n",
    "        \"\"\"Gera captions usando Florence-2\"\"\"\n",
    "        if not self.florence_model or not self.florence_processor:\n",
    "            logger.warning(\"Florence-2 model not loaded, returning empty results\")\n",
    "            return {}\n",
    "        \n",
    "        prompts = [\n",
    "            \"<CAPTION>\",\n",
    "            \"<DETAILED_CAPTION>\", \n",
    "            \"<MORE_DETAILED_CAPTION>\",\n",
    "            \"<DENSE_REGION_CAPTION>\",\n",
    "            \"<OD>\",\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                inputs = self.florence_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "                generated_ids = self.florence_model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    pixel_values=inputs[\"pixel_values\"],\n",
    "                    max_new_tokens=max_new_tokens\n",
    "                )\n",
    "                generated_text = self.florence_processor.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=False\n",
    "                )[0]\n",
    "                parsed_answer = self.florence_processor.post_process_generation(\n",
    "                    generated_text,\n",
    "                    task=prompt,\n",
    "                    image_size=(image.width, image.height)\n",
    "                )\n",
    "                results[prompt] = parsed_answer.get(prompt, \"\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro no Florence-2 com prompt {prompt}: {e}\")\n",
    "                results[prompt] = \"\"\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_grounding_detection(self, image: Image.Image, \n",
    "                              text_prompt: str = \"oil spill\",\n",
    "                              box_threshold: float = 0.3,\n",
    "                              text_threshold: float = 0.2) -> Dict:\n",
    "        \"\"\"Processa detecção com GroundingDINO\"\"\"\n",
    "        try:\n",
    "            if not self.grounding_model:\n",
    "                logger.warning(\"GroundingDINO model not loaded, returning empty results\")\n",
    "                return {\n",
    "                    'annotated_image': np.array(image),\n",
    "                    'detection_data': {\n",
    "                        'boxes': np.array([]),\n",
    "                        'logits': np.array([]),\n",
    "                        'phrases': [],\n",
    "                        'image_dimensions': (image.height, image.width)\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Converter PIL Image para numpy array\n",
    "            image_array = np.array(image)\n",
    "            \n",
    "            # Converter imagem para tensor\n",
    "            image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()\n",
    "            \n",
    "            # Processar detecção\n",
    "            self.grounding_model.eval()\n",
    "            with torch.no_grad():\n",
    "                boxes, logits, phrases = predict(\n",
    "                    model=self.grounding_model,\n",
    "                    image=image_tensor,\n",
    "                    caption=text_prompt,\n",
    "                    box_threshold=box_threshold,\n",
    "                    text_threshold=text_threshold,\n",
    "                    device='cpu'\n",
    "                )\n",
    "            \n",
    "            # Converter tensores para numpy\n",
    "            boxes_np = boxes.cpu().numpy()\n",
    "            logits_np = logits.cpu().numpy()\n",
    "            \n",
    "            # Anotar a imagem\n",
    "            annotated_frame = annotate(image_array, boxes, logits, phrases)\n",
    "            \n",
    "            return {\n",
    "                'annotated_image': annotated_frame,\n",
    "                'detection_data': {\n",
    "                    'boxes': boxes_np,\n",
    "                    'logits': logits_np,\n",
    "                    'phrases': phrases,\n",
    "                    'image_dimensions': (image.height, image.width)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no GroundingDINO: {e}\")\n",
    "            # Retornar estrutura vazia em caso de erro\n",
    "            return {\n",
    "                'annotated_image': np.array(image),\n",
    "                'detection_data': {\n",
    "                    'boxes': np.array([]),\n",
    "                    'logits': np.array([]),\n",
    "                    'phrases': [],\n",
    "                    'image_dimensions': (image.height, image.width)\n",
    "                }\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OilDetectionServer:\n",
    "    \"\"\"Servidor principal para detecção de óleo\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        self.db_manager = DatabaseManager()\n",
    "        self.file_manager = FileManager()\n",
    "        self.model_manager = ModelManager()\n",
    "        self._setup_routes()\n",
    "    \n",
    "    def _setup_routes(self):\n",
    "        \"\"\"Configura as rotas do Flask\"\"\"\n",
    "        \n",
    "        @self.app.route('/oil_detection', methods=['POST'])\n",
    "        def oil_detection_endpoint():\n",
    "            return self.process_oil_detection()\n",
    "        \n",
    "        @self.app.route('/processar', methods=['POST'])\n",
    "        def processar():\n",
    "            return self.legacy_processar()\n",
    "        \n",
    "        @self.app.route('/skf_analysis', methods=['POST'])\n",
    "        def handle_skf_analysis():\n",
    "            return self.skf_analysis_endpoint()\n",
    "        \n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            return jsonify({\"status\": \"healthy\", \"timestamp\": datetime.now().isoformat()})\n",
    "        \n",
    "        @self.app.route('/stats', methods=['GET'])\n",
    "        def get_stats():\n",
    "            return self.get_detection_stats()\n",
    "        \n",
    "        @self.app.route('/stats/<empresa_id>', methods=['GET'])\n",
    "        def get_empresa_stats(empresa_id):\n",
    "            return self.get_empresa_stats(empresa_id)\n",
    "        \n",
    "        @self.app.route('/detection/<empresa_id>/<int:deteccao_id>', methods=['GET'])\n",
    "        def get_detection_details(empresa_id, deteccao_id):\n",
    "            return self.get_detection_details(empresa_id, deteccao_id)\n",
    "    \n",
    "    def process_oil_detection(self):\n",
    "        \"\"\"Endpoint principal para processamento de detecção de óleo\"\"\"\n",
    "        try:\n",
    "            data = request.get_json()\n",
    "            \n",
    "            # Validar dados obrigatórios\n",
    "            required_fields = ['imagem_original_base64', 'empresa_info']\n",
    "            for field in required_fields:\n",
    "                if field not in data:\n",
    "                    return jsonify({\n",
    "                        \"status\": \"error\", \n",
    "                        \"message\": f\"Campo obrigatório ausente: {field}\"\n",
    "                    }), 400\n",
    "            \n",
    "            # Extrair dados da estrutura empresa_info\n",
    "            empresa_info = data['empresa_info']\n",
    "            empresa_id = empresa_info['empresa_id']\n",
    "            empresa_nome = empresa_info['empresa_nome']\n",
    "            coletor_id = empresa_info['coletor_id']\n",
    "            coletor_descricao = empresa_info.get('coletor_descricao', '')\n",
    "            coletor_localizacao = empresa_info.get('localizacao', '')\n",
    "            modelo_usado = empresa_info.get('modelo_id', 'oil_spill')\n",
    "            confidence_yolo = data.get('confidence_threshold')\n",
    "            timestamp_coleta = data.get('timestamp', datetime.now().isoformat())\n",
    "            \n",
    "            # Validação e conversão segura da imagem\n",
    "            try:\n",
    "                imagem_bytes = base64.b64decode(data['imagem_original_base64'])\n",
    "                image = Image.open(io.BytesIO(imagem_bytes)).convert(\"RGB\")\n",
    "                logger.info(f\"Image loaded successfully: {image.size}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro ao processar imagem base64: {e}\")\n",
    "                return jsonify({\n",
    "                    \"status\": \"error\", \n",
    "                    \"message\": f\"Erro ao processar imagem: {str(e)}\"\n",
    "                }), 400\n",
    "            \n",
    "            # Inserir/atualizar empresa e coletor no banco específico da empresa\n",
    "            self.db_manager.insert_empresa(empresa_id, empresa_nome)\n",
    "            self.db_manager.insert_coletor(empresa_id, coletor_id, coletor_descricao, coletor_localizacao)\n",
    "            \n",
    "            # Criar estrutura de diretórios específica da empresa\n",
    "            raw_dir, processed_dir = self.file_manager.create_directory_structure(\n",
    "                empresa_id, coletor_id, modelo_usado\n",
    "            )\n",
    "            \n",
    "            # Processar com Florence-2\n",
    "            logger.info(f\"Processing image with Florence-2 for {empresa_id}/{coletor_id}\")\n",
    "            florence_results = self.model_manager.generate_florence_captions(image)\n",
    "            \n",
    "            # Processar com GroundingDINO\n",
    "            logger.info(f\"Processing image with GroundingDINO for {empresa_id}/{coletor_id}\")\n",
    "            grounding_results = self.model_manager.process_grounding_detection(image)\n",
    "            \n",
    "            # Calcular detecções de forma segura\n",
    "            detection_data = grounding_results.get('detection_data', {})\n",
    "            logits = detection_data.get('logits', np.array([]))\n",
    "            \n",
    "            # Usar contagem de detecções do payload se existir, senão calcular\n",
    "            if isinstance(logits, np.ndarray):\n",
    "                objects_detected = data.get('detections_count', int(logits.size))\n",
    "            else:\n",
    "                objects_detected = data.get('detections_count', len(logits) if logits else 0)\n",
    "            \n",
    "            # Gerar nomes de arquivo\n",
    "            timestamp_str = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "            raw_filename = data.get('name_imagem_original', \n",
    "                                  self.file_manager.generate_filename(\n",
    "                                      empresa_id, coletor_id, timestamp_str, modelo_usado, objects_detected, \"RAW\"\n",
    "                                  ))\n",
    "            processed_filename = data.get('name_imagem_processada',\n",
    "                                        self.file_manager.generate_filename(\n",
    "                                            empresa_id, coletor_id, timestamp_str, modelo_usado, objects_detected, \"PROC\"\n",
    "                                        ))\n",
    "            \n",
    "            # Salvar imagens\n",
    "            raw_path = raw_dir / raw_filename\n",
    "            processed_path = processed_dir / processed_filename\n",
    "            \n",
    "            # Verificar se salvou corretamente\n",
    "            if not self.file_manager.save_image(image, raw_path):\n",
    "                logger.warning(f\"Failed to save raw image to {raw_path}\")\n",
    "            \n",
    "            # Salvar imagem processada se existir no payload\n",
    "            processed_image_saved = False\n",
    "            if 'imagem_processada_base64' in data:\n",
    "                try:\n",
    "                    processed_bytes = base64.b64decode(data['imagem_processada_base64'])\n",
    "                    processed_image = Image.open(io.BytesIO(processed_bytes)).convert(\"RGB\")\n",
    "                    processed_image_saved = self.file_manager.save_image(processed_image, processed_path)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erro ao processar imagem processada: {e}\")\n",
    "            elif 'annotated_image' in grounding_results:\n",
    "                try:\n",
    "                    annotated_array = grounding_results['annotated_image']\n",
    "                    if isinstance(annotated_array, np.ndarray) and annotated_array.size > 0:\n",
    "                        processed_image = Image.fromarray(annotated_array.astype('uint8'))\n",
    "                        processed_image_saved = self.file_manager.save_image(processed_image, processed_path)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erro ao salvar imagem anotada: {e}\")\n",
    "            \n",
    "            # Preparar dados para inserção no banco\n",
    "            dados_deteccao = {\n",
    "                'coletor_id': coletor_id,\n",
    "                'timestamp_coleta': timestamp_coleta,\n",
    "                'modelo_usado': modelo_usado,\n",
    "                'imagem_original_path': str(raw_path),\n",
    "                'imagem_processada_path': str(processed_path) if processed_image_saved else None,\n",
    "                'confidence_yolo': confidence_yolo,\n",
    "                'objects_detected_yolo': objects_detected\n",
    "            }\n",
    "            \n",
    "            # Inserir no banco de dados específico da empresa\n",
    "            deteccao_id = self.db_manager.insert_deteccao(empresa_id, dados_deteccao)\n",
    "            self.db_manager.insert_florence_result(empresa_id, deteccao_id, florence_results)\n",
    "            self.db_manager.insert_grounding_result(empresa_id, deteccao_id, grounding_results)\n",
    "            \n",
    "            # Calcular max_confidence de forma segura\n",
    "            if isinstance(logits, np.ndarray):\n",
    "                max_confidence = float(np.max(logits)) if logits.size > 0 else 0.0\n",
    "            else:\n",
    "                max_confidence = float(max(logits)) if logits else 0.0\n",
    "            \n",
    "            # Preparar resposta\n",
    "            response = {\n",
    "                \"status\": \"success\",\n",
    "                \"deteccao_id\": deteccao_id,\n",
    "                \"empresa_id\": empresa_id,\n",
    "                \"objects_detected\": objects_detected,\n",
    "                \"oil_spill_detected\": objects_detected > 0,\n",
    "                \"max_confidence\": max_confidence,\n",
    "                \"florence_caption\": florence_results.get('<CAPTION>', ''),\n",
    "                \"files_saved\": {\n",
    "                    \"raw_image\": str(raw_path),\n",
    "                    \"processed_image\": str(processed_path) if processed_image_saved else None\n",
    "                },\n",
    "                \"timestamp_processamento\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Successfully processed detection {deteccao_id} for {empresa_id}/{coletor_id}\")\n",
    "            return jsonify(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no processamento: {e}\", exc_info=True)\n",
    "            return jsonify({\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Erro interno do servidor: {str(e)}\"\n",
    "            }), 500\n",
    "        \n",
    "    def legacy_processar(self):\n",
    "        \"\"\"Endpoint legacy para compatibilidade\"\"\"\n",
    "        try:\n",
    "            dados = request.get_json()\n",
    "            imagem_b64 = dados.get('imagem_base64')\n",
    "            nome_imagem = dados.get('nome_imagem', 'imagem_recebida.jpg')\n",
    "            texto = dados.get('texto', '')\n",
    "            \n",
    "            if not imagem_b64:\n",
    "                return jsonify({\"resposta\": \"Imagem base64 não fornecida\"}), 400\n",
    "            \n",
    "            # Salva a imagem temporariamente\n",
    "            with open(nome_imagem, \"wb\") as f:\n",
    "                f.write(base64.b64decode(imagem_b64))\n",
    "            \n",
    "            tamanho_kb = os.path.getsize(nome_imagem) / 1024\n",
    "            resposta = f\"{nome_imagem}: {tamanho_kb:.1f}kb | Texto recebido: {texto}\"\n",
    "            \n",
    "            return jsonify({\"resposta\": resposta})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no endpoint legacy: {e}\")\n",
    "            return jsonify({\"resposta\": f\"Erro: {str(e)}\"}), 500\n",
    "    \n",
    "    def skf_analysis_endpoint(self):\n",
    "        \"\"\"Endpoint para análise SKF (Florence-2)\"\"\"\n",
    "        try:\n",
    "            data = request.json\n",
    "            \n",
    "            imagem_base64 = data.get(\"imagem_base64\")\n",
    "            if not imagem_base64:\n",
    "                return jsonify({\"status\": \"error\", \"message\": \"Imagem base64 não fornecida\"}), 400\n",
    "            \n",
    "            # Converter base64 para imagem\n",
    "            imagem_bytes = base64.b64decode(imagem_base64)\n",
    "            image_stream = io.BytesIO(imagem_bytes)\n",
    "            image = Image.open(image_stream).convert(\"RGB\")\n",
    "            \n",
    "            # Processar com Florence-2\n",
    "            result = self.model_manager.generate_florence_captions(image)\n",
    "            \n",
    "            return jsonify({\n",
    "                \"status\": \"success\",\n",
    "                \"return\": result\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no SKF analysis: {e}\")\n",
    "            return jsonify({\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Erro no servidor: {str(e)}\"\n",
    "            }), 500\n",
    "    \n",
    "    def get_detection_stats(self):\n",
    "        \"\"\"Retorna estatísticas globais de todas as empresas\"\"\"\n",
    "        try:\n",
    "            # Buscar todas as empresas no diretório base\n",
    "            base_path = Path(self.db_manager.base_path)\n",
    "            if not base_path.exists():\n",
    "                return jsonify({\n",
    "                    \"status\": \"success\",\n",
    "                    \"stats\": {\n",
    "                        \"total_empresas\": 0,\n",
    "                        \"total_deteccoes\": 0,\n",
    "                        \"deteccoes_positivas\": 0,\n",
    "                        \"taxa_deteccao\": 0,\n",
    "                        \"empresas\": []\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            empresas_stats = []\n",
    "            total_deteccoes = 0\n",
    "            total_positivas = 0\n",
    "            \n",
    "            for empresa_dir in base_path.iterdir():\n",
    "                if empresa_dir.is_dir():\n",
    "                    empresa_id = empresa_dir.name\n",
    "                    stats = self.db_manager.get_empresa_stats(empresa_id)\n",
    "                    \n",
    "                    if \"error\" not in stats:\n",
    "                        empresas_stats.append(stats)\n",
    "                        total_deteccoes += stats[\"total_deteccoes\"]\n",
    "                        total_positivas += stats[\"deteccoes_positivas\"]\n",
    "            \n",
    "            return jsonify({\n",
    "                \"status\": \"success\",\n",
    "                \"stats\": {\n",
    "                    \"total_empresas\": len(empresas_stats),\n",
    "                    \"total_deteccoes\": total_deteccoes,\n",
    "                    \"deteccoes_positivas\": total_positivas,\n",
    "                    \"taxa_deteccao\": (total_positivas / total_deteccoes * 100) if total_deteccoes > 0 else 0,\n",
    "                    \"empresas\": empresas_stats\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao obter estatísticas globais: {e}\")\n",
    "            return jsonify({\n",
    "                \"status\": \"error\",\n",
    "                \"message\": str(e)\n",
    "            }), 500\n",
    "    \n",
    "    def get_empresa_stats(self, empresa_id: str):\n",
    "        \"\"\"Retorna estatísticas específicas de uma empresa\"\"\"\n",
    "        try:\n",
    "            stats = self.db_manager.get_empresa_stats(empresa_id)\n",
    "            \n",
    "            if \"error\" in stats:\n",
    "                return jsonify({\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": stats[\"error\"]\n",
    "                }), 404\n",
    "            \n",
    "            # Adicionar informações de diretório\n",
    "            dir_info = self.file_manager.get_empresa_directory_info(empresa_id)\n",
    "            if \"error\" not in dir_info:\n",
    "                stats[\"directory_info\"] = dir_info\n",
    "            \n",
    "            return jsonify({\n",
    "                \"status\": \"success\",\n",
    "                \"stats\": stats\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao obter estatísticas da empresa {empresa_id}: {e}\")\n",
    "            return jsonify({\n",
    "                \"status\": \"error\",\n",
    "                \"message\": str(e)\n",
    "            }), 500\n",
    "    \n",
    "    def get_detection_details(self, empresa_id: str, deteccao_id: int):\n",
    "        \"\"\"Retorna detalhes completos de uma detecção específica\"\"\"\n",
    "        try:\n",
    "            detection_data = self.db_manager.get_detection_with_details(empresa_id, deteccao_id)\n",
    "            \n",
    "            if not detection_data:\n",
    "                return jsonify({\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": \"Detecção não encontrada\"\n",
    "                }), 404\n",
    "            \n",
    "            return jsonify({\n",
    "                \"status\": \"success\",\n",
    "                \"data\": detection_data\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao obter detalhes da detecção {deteccao_id} da empresa {empresa_id}: {e}\")\n",
    "            return jsonify({\n",
    "                \"status\": \"error\",\n",
    "                \"message\": str(e)\n",
    "            }), 500\n",
    "    \n",
    "    def run(self, host=\"0.0.0.0\", port=8080, debug=False):\n",
    "        \"\"\"Executa o servidor\"\"\"\n",
    "        logger.info(f\"Starting Oil Detection Server on {host}:{port}\")\n",
    "        self.app.run(host=host, port=port, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 18:49:45,737 - INFO - Loading Florence-2 model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Oil Detection Server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SyntaxWarning: invalid escape sequence '\\d'\n",
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "2025-06-14 18:50:25,964 - INFO - Florence-2 model loaded successfully\n",
      "2025-06-14 18:50:25,966 - INFO - Loading GroundingDINO model...\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4316.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 18:50:40,431 - INFO - GroundingDINO model loaded successfully\n",
      "2025-06-14 18:50:40,434 - INFO - Starting Oil Detection Server on 0.0.0.0:8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 18:50:40,440 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://10.2.0.2:8080\n",
      "2025-06-14 18:50:40,441 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2025-06-14 18:51:11,350 - INFO - 127.0.0.1 - - [14/Jun/2025 18:51:11] \"GET /health HTTP/1.1\" 200 -\n",
      "2025-06-14 18:51:23,576 - INFO - Image loaded successfully: (640, 480)\n",
      "2025-06-14 18:51:23,752 - INFO - Normalized database initialized successfully for empresa: DOS0724\n",
      "2025-06-14 18:51:23,773 - INFO - Processing image with Florence-2 for DOS0724/APS005\n",
      "2025-06-14 18:51:34,129 - INFO - Processing image with GroundingDINO for DOS0724/APS005\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "2025-06-14 18:51:36,439 - INFO - Successfully processed detection 1 for DOS0724/APS005\n",
      "2025-06-14 18:51:36,440 - INFO - 127.0.0.1 - - [14/Jun/2025 18:51:36] \"POST /oil_detection HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Verificar se estamos no processo principal (evitar problemas com reloader)\n",
    "    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true': print(\"Initializing Oil Detection Server...\")\n",
    "    \n",
    "    server = OilDetectionServer()\n",
    "        \n",
    "    # Executar com debug desabilitado ou com reloader desabilitado\n",
    "    debug_mode = \"--debug\" in sys.argv\n",
    "    server.run(debug=debug_mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
